{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algarve piezometric values for all aquifer locations\n",
    "* Notebook adapted from great works of Luis Costa from APA\n",
    "* single station data from `aww_output/single_station_data` are linked `aww_output/algarve_stations.csv` table via `site_id`\n",
    "\n",
    "TODO: convert this to two scripts\n",
    "1. runs only once upon a time to confirm station updates etc\n",
    "2. runs every month to get new data\n",
    "    * add asserts\n",
    "    * update existing data, rather then download all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for requests\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/118.0\",\n",
    "}\n",
    "\n",
    "# Function to get the representative piezometer name, from the dict\n",
    "def get_key(d: dict, val: str):\n",
    "    for key, value in d.items(): # give name of piezometer, get code\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Creating a dictionary of station IDs and code\n",
    "\n",
    "Query from website with station info (including Id and code) have been stored at \"station_list.csv\"\n",
    "\n",
    "Stations list: https://snirh.apambiente.pt/snirh/_dadosbase/site/xml/xml_listaestacoes.php\n",
    "\n",
    "Info to retreive\n",
    "marker site -> hiperlink code\n",
    "estacao -> Station ID\n",
    "lat -> Latitude\n",
    "lng -> Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - reading the data and converting it to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station = pd.read_csv('input/station_list.csv')\n",
    "df_station = df_station[\"<markers>\"].str.split('\"', expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols to drop\n",
    "cols_drop = [0,2,3,4,5,6,8,10,11,12,14,16,17,18,19,20]\n",
    "df_station.drop(df_station.columns[cols_drop],axis=1,inplace=True)\n",
    "\n",
    "# Change the column names\n",
    "df_station.columns =['marker site','latitude','longitude','estacao3','estacao']\n",
    "\n",
    "# marker site is the hyperlink in the website SNIRH to reach each station\n",
    "\n",
    "#clean the estacao3 and estacao fields\n",
    "df_station['estacao3'] = df_station['estacao3'].str.replace('&amp;#9632; ','')\n",
    "df_station['estacao'] = df_station['estacao'].str.replace('■ ','')\n",
    "df_station.drop(['estacao3'], axis=1, inplace=True)\n",
    "\n",
    "df_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_station.to_csv('aww_output/station_list_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Creating a dataframe with the dictionary and all the relevant information of the observation stations \n",
    "\n",
    "Create a column with the corresponding \"aquifer\", a column with the \"Região Hidrográfica\", a column with the \"altitude\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the list of station and info from csv in pc (same as below, but retrieiving data from computer instead of internet)\n",
    "\n",
    "df_local = pd.read_csv('input/rede_Piezometria.csv', encoding = \"ISO-8859-1\", skiprows=3, index_col=False)\n",
    "df_local.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this retrieves an empty table, therefore below I use the df2_local\n",
    "\n",
    "# url2 = 'https://snirh.apambiente.pt/snirh/_dadosbase/site/paraCSV/lista_csv.php?obj_janela=INFO_ESTACOES&s_cover=100290946&tp_lista=&completa=1&formato=csv'\n",
    "\n",
    "# r = requests.get(url2, headers=headers, allow_redirects=True).content # getting the file\n",
    "# df_web = pd.read_csv(io.StringIO(r.decode(\"ISO-8859-1\")), skiprows=3, index_col=False)\n",
    "\n",
    "# df_web.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the df with the hyperlink code to the df with the stations caracteristics\n",
    "\n",
    "df_stations = pd.merge(df_local, df_station,\n",
    "                       left_on='CÓDIGO', right_on='estacao',\n",
    "                       how='outer', indicator=True) #merge the df with hyperlink - station relation, with the df with info on the stations\n",
    "df_stations = df_stations.iloc[:-1 , :] #drop the last row which is uneeded info\n",
    "df_stations = df_stations[df_stations['_merge'] != 'left_only'] # to remove obs point 607/884 that was added to the list, but was abandanoed, and it does not show up in the XML list\n",
    "\n",
    "df_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Setting the filter for the zones in PT. \n",
    "* In this case, we want  only Algarve\n",
    "* select piezometers based on \"BACIA\"\n",
    "* save DF to csv: `algarve_stations.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a df with all the river basins covering Algarve, so we can select all obs pointsfrom it after.\n",
    "Bacias = df_local['BACIA'].unique() #view all the available \"BACIAS\"\n",
    "Bacias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"BACIAS\" for Algarve: 'RIBEIRAS DO ALGARVE','GUADIANA','ARADE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: site_id should be made into index I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting all sites of the Algarve, considering the ones selected in the Algarve \"BACIAS\"\n",
    "sites_Algarve = df_stations.loc[df_stations['BACIA'].isin(['RIBEIRAS DO ALGARVE','GUADIANA','ARADE'])]\n",
    "\n",
    "# remove duplicate columns or columns with no value\n",
    "sites_Algarve = sites_Algarve.drop([\"_merge\", \"CÓDIGO\"], axis=1)\n",
    "sites_Algarve = sites_Algarve.rename(columns={\"estacao\": \"site_id\"})\n",
    "\n",
    "# save and show\n",
    "# sites_Algarve.to_csv(\"aww_output/algarve_stations.csv\", header = True)\n",
    "sites_Algarve.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Getting the data with a For Loop to create an individual csv file for each Obs Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with obs points in Sites_Algarve \n",
    "station_Algarve_dict = dict(zip(sites_Algarve['marker site'], sites_Algarve['site_id']))\n",
    "\n",
    "# dictionary with obs points code and belonging aquifer\n",
    "station_aquif_Algarve_dict = dict(zip(sites_Algarve['marker site'], sites_Algarve['SISTEMA AQUÍFERO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filter Parameters in this case hydrological years 1990 until 2025\n",
    "iDate = '01/10/1990' #Initial date\n",
    "eDate = '28/05/2025' #End date\n",
    "par = '2277' #depth to groundwater level\n",
    "print(f\"number of stations to be saved (1 csv per station): {len(list(station_aquif_Algarve_dict.keys()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(list(station_aquif_Algarve_dict.keys())): #for all obs points within the selected aquifers\n",
    "    PiezometerName = station_Algarve_dict[key]\n",
    "    PiezometerCode = get_key(station_Algarve_dict, PiezometerName)\n",
    "\n",
    "    url = 'https://snirh.apambiente.pt/snirh/_dadosbase/site/paraCSV/dados_csv.php?sites='+key+'&pars='+par+'&tmin=' + iDate + '&tmax=' + eDate + '&formato=csv'\n",
    "    r = requests.get(url, headers=headers, allow_redirects=True).content # getting the file\n",
    "    df = pd.read_csv(io.StringIO(r.decode(\"ISO-8859-1\")), skiprows=3, dayfirst=True,\n",
    "                     parse_dates = [0], index_col = [0], usecols = [0,1], header = 0, skipfooter = 1,\n",
    "                     names = ['Date', PiezometerName], engine = 'python') \n",
    "    \n",
    "    filepath = './aww_output/single_station_data/' + PiezometerCode + '.csv'\n",
    "    df.to_csv(filepath, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example code of how to query the data from SNIRH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iDate = '01/01/1990' #Initial date\n",
    "# eDate = '31/12/1995' #End date\n",
    "# sites = '107724,107731,107753,2047288,71842,2046000,2077730,107825,106626,109128,107077,107073,107074,107098,107099,107066,71844,107119,107519,106628,107259,107557,107260,107272,107233,107560,107219,107252,107251,107240,107550,107340' #hiperlink code for the obs points\n",
    "# pars = '100290981' #piezometric level \n",
    "\n",
    "# # url def \n",
    "# url = 'https://snirh.apambiente.pt/snirh/_dadosbase/site/paraCSV/dados_csv.php?sites=' + sites + '&pars=' + pars + '&tmin=' + iDate + '&tmax=' + eDate + '&formato=csv'\n",
    "# # in url, \n",
    "# #       sites=... selects the observation points based on their hyperlink code; \n",
    "# #       par = selection of the parameter we want to download (piezometria ou profunidade ao nível) based on the specific hyperlink code\n",
    "# #       tmin and tmax = initial and end date\n",
    "\n",
    "\n",
    "# r = requests.get(url, headers=headers, allow_redirects=True).content # getting the file\n",
    "# df_test = pd.read_csv(io.StringIO(r.decode(\"ISO-8859-1\")), skiprows=3)\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.drop(df_test.tail(1).index, inplace = True) #dropping last row with unwanted text\n",
    "# df_test = df_test.rename(columns={'Unnamed: 0': 'Date'}) #renaming dateindex column\n",
    "# df_test = df_test.drop(['Unnamed: 65'],axis=1) \n",
    "# df_test = df_test.set_index('Date') #transforming into dateindex\n",
    "# df_test = df_test.drop(df_test.filter(regex='FLAG').columns, axis=1) #dropping blank columns - Attention this is should only be done after elimination of \"<\" or \">\" values of the corrseponding observation point. \n",
    "# df_test.columns = ['606/1026','606/1033','606/1057','606/1118','606/1122','606/1461','606/1639','606/167',\n",
    "#               '606/647','607/498','610/167','610/179','610/180',\n",
    "#               '610/182','610/183','610/186','610/213','610/6',\n",
    "#               '610/65','611/115','611/155','611/182','611/200',\n",
    "#               '611/202','611/209','611/217','611/230','611/233',\n",
    "#               '611/234','611/236','611/237','611/91']\n",
    "\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aww_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
